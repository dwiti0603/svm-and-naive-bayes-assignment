{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**SVM and Naives Bayes Assignment**"
      ],
      "metadata": {
        "id": "ezREB9N9uY5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Ans1. Information Gain (IG) is a measure from information theory that quantifies how much “information” (in the sense of reduction in uncertainty) you get about a target variable by knowing the value of some attribute. Information Gain in a decision tree is used as a criterion to pick which feature to split on at each node: you compute the decrease in **uncertainty (entropy)** about the target class when you split the data based on a particular feature, and choose the feature that yields the **largest drop** (i.e. the highest information gain).\n",
        "\n",
        "Q2. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Ans2. Gini Impurity measures how often a randomly chosen item from the node would be misclassified if it were labeled according to the class proportions (lower is purer).  Entropy, drawn from information theory, measures the amount of uncertainty or “disorder” in the class distribution (zero when all items are the same class, higher when mixed). Gini is computationally simpler (no logarithms) and often results in slightly faster splitting, whereas Entropy is more sensitive to changes in class probabilities and sometimes yields more balanced splits.\n",
        "\n",
        "Q3. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Ans4. Pre-Pruning (also called early stopping) is a technique used during the construction of a decision tree: instead of letting the tree grow until every leaf is “pure,” you impose constraints (stopping criteria) so that some branches are not grown if they don’t satisfy certain rules.\n",
        "\n"
      ],
      "metadata": {
        "id": "qeKJaj_RuZbb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwWCeBIeuXAL",
        "outputId": "321a1708-2077-4559-9db8-c432c1c2a92f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances (using Gini):\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n",
            "\n",
            "Tree depth: 6\n",
            "Number of leaves: 10\n"
          ]
        }
      ],
      "source": [
        "#Q4. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "#Ans4.\n",
        "\n",
        "# Example: Decision Tree Classifier (Gini) + Feature Importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 1. Load dataset (for demonstration, using Iris dataset)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Split into train and test (optional but a good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Create and train the classifier with Gini impurity criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Print feature importances\n",
        "importances = clf.feature_importances_\n",
        "print(\"Feature importances (using Gini):\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# Optional: show some model details\n",
        "print(\"\\nTree depth:\", clf.get_depth())\n",
        "print(\"Number of leaves:\", clf.get_n_leaves())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Ans5. A Support Vector Machine is a supervised machine-learning algorithm that finds a hyperplane (in a space defined by the features) that best separates data points of different classes. It chooses the hyperplane that maximizes the “margin” — the distance between the hyperplane and the nearest data points of any class (the support vectors), which helps SVM generalize well to unseen data.\n",
        "\n",
        "Q6. What is the Kernel Trick in SVM?\n",
        "\n",
        "Ans6. The Kernel Trick is a technique used by Support Vector Machine to handle data that are **not linearly separable** in the original feature space. Instead of explicitly transforming the data into a higher-dimensional space (which could be computationally expensive or even infinite-dimensional), SVM uses a **kernel function** that computes the dot product of the data points as if they were in that higher-dimensional space — all without explicitly doing the transformation. This “trick” allows the SVM to define a **linear hyperplane in a transformed (higher-dimensional) space**, which when projected back corresponds to a **non-linear decision boundary in the original space** — enabling SVMs to classify complex, non-linear data efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "_iS7r_K9w9Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "#Ans7.\n",
        "\n",
        "# SVM on Wine dataset: compare Linear vs RBF kernel\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the data\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Feature scaling — important for SVM\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# 5. Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# 6. Print accuracies\n",
        "print(f\"Accuracy (Linear kernel): {acc_linear * 100:.2f}%\")\n",
        "print(f\"Accuracy (RBF kernel):    {acc_rbf * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUOGpL-_yCzp",
        "outputId": "0b7f407c-1798-400f-b5a4-207569d37fea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Linear kernel): 96.30%\n",
            "Accuracy (RBF kernel):    98.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Ans8. The Naive Bayes classifier is a simple probabilistic classification algorithm that uses Bayes' Theorem to compute the probability that a given example belongs to each class, based on its features.It is called “naïve” because the method makes a strong simplification: it assumes that all features are conditionally independent of each other, given the class label. That is, it treats each feature as if it contributes on its own to the probability of the class — even if in reality features are often correlated. This assumption is often unrealistic (hence “naïve”), but it makes the model very simple and computationally efficient — which is also why Naive Bayes works well in many real-world tasks (especially those with many features, like text classification).\n",
        "\n",
        "Q9.  Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "Ans9. Gaussian NB assumes your features are continuous and follow a normal (Gaussian) distribution, so it works best with real-valued numeric data.\n",
        "Multinomial NB expects discrete features representing counts or frequencies (e.g. word counts in documents) — and uses a multinomial distribution to model them. Bernoulli NB, by contrast, assumes binary features (presence/absence of an attribute), making it suited for data where you only care whether something occurs (or not), not how often."
      ],
      "metadata": {
        "id": "tbGrCrwbyVJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10.  Breast Cancer Dataset : Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "#Ans10.\n",
        "\n",
        "# Gaussian Naive Bayes on Breast Cancer dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Create and train GaussianNB classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on test data: {accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDxVhIrozWSh",
        "outputId": "f3b61df4-2025-44f3-ac1d-813274d6800d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 94.74%\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.97      0.89      0.93        64\n",
            "      benign       0.94      0.98      0.96       107\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.95      0.94      0.94       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n",
            "Confusion matrix:\n",
            "[[ 57   7]\n",
            " [  2 105]]\n"
          ]
        }
      ]
    }
  ]
}